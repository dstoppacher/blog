---
layout: post
title:  "Pleiades"
date:   2020-03-19

categories: cosmo_sims
---

I will be running things on <a href="https://www.nas.nasa.gov/hecc/resources/pleiades.html">Pleiades</a>. Documentation on using this system can be found  <a href="https://www.nas.nasa.gov/hecc/support/kb/">here</a>, but in this post I will write my notes on getting everything working.

## Help

The NAS Control Room staff are available 24/7 and can be contacted by phone at 650-604-4444 or through email at support@nas.nasa.gov.


## Logging On

There is information on how to log-on to Pleiades
<a href="https://www.nas.nasa.gov/hecc/support/kb/91/">here</a>.

In summary:

1) ssh into username@sfe1.nas.nasa.gov first (PAM authentication refers to the password generated by the soft token---the pin is an 8 digit number)

2) ssh into username@pfe (this is the Pleiades Front-End Load Balancer)

There are instructions in the link above for setting up a one-step connection, which I will want to do eventually.

## Allocations/Quotas

### Usage

You can track your jobs and allocations on the <a href="https://portal.nas.nasa.gov">myNAS web portal</a>.

In the terminal, you can also check your <a href-="https://www.nas.nasa.gov/hecc/support/kb/job-accounting_171.htmlstandard">usage </a> with the command
<code>acct_ytd</code>.

### Storage

Information on quotas can be found
 <a href-="https://www.nas.nasa.gov/hecc/support/kb/quota-policy-on-disk-space-and-files_156.html">here</a>, and you can check your quota using <code>quota -v</code>.

By default you get 8 GB on your home directory, but you can try and request more by emailing support@nas.nasa.gov.

For short-term storage you get 1 TB on the Lustre/nobackup filesystems.

For long-term storage you can use the <a href="https://www.nas.nasa.gov/hecc/support/kb/the-lou-mass-storage-system_371.html">Lou Mass Storage System</a>, which has no disk quota limits.




## Running Jobs

The HECC supercomputers use the Portable Batch System (PBS) to schedule jobs.

### Submit a Job

The following command is used to submit jobs:

<code>
%qsub job_script
</code>

There is a sample job_script <a href="https://www.nas.nasa.gov/hecc/support/kb/sample-pbs-script-for-pleiades_190.html">here</a>.


### Queues

The <code>normal</code>, <code>long</code> and <code>low</code> queues are for production work, while the <code>debug</code> and <code>devel</code> queues are for debugging and development. For the <code>debug</code> queue you can have a maximum two running jobs with a total of 128 nodes, while in the <code>devel</code> queue maximum one job at a time, maximum wall-clock time of 2 hours and 512 nodes.



### Monitoring Jobs

The command for checking the status of all your jobs is  <code>qstat -nu username</code>. For the full status of a job 12345, you use <code>qstat -f 12345</code>.

You can delete a job using <code>qdel 12345</code> and hold/release a job with <code>qhold 12345</code> and <code>qrls 12345</code>.



## Test Run

### Install Gadget2

As usual, I will follow <a href="https://astrobites.org/2011/04/02/installing-and-running-gadget-2/">these</a> notes for setting up <span style="font-variant:small-caps;">gadget-2</span>.

Note that you have to load an mpi module to install fftw (use <code>module load package</code>; you can check available modules using <code>module avail</code>).

### Job Script

Here is my job script:

<object width="500" height="300" type="text/plain" data="{{site.baseurl}}/assets/files/run_wfirst128.conf" border="0" >
</object>

***STILL DEBUGGING THIS***
